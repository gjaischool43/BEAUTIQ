"""
YouTube Creator Analysis System - ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ (ìˆ˜ì •ë³¸)
- ê° ì˜ìƒì˜ ëŒ“ê¸€ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ê¸°ëŠ¥ ì¶”ê°€
- @usernameìœ¼ë¡œ ì±„ë„ ID ê²€ìƒ‰ ê¸°ëŠ¥ í¬í•¨
"""

import os
import json
from datetime import datetime, timedelta
from googleapiclient.discovery import build # pip install google-api-python-client
from googleapiclient.errors import HttpError
import isodate # pip install isodate

class YouTubeDataCollector:
    def __init__(self, api_key):
        """
        YouTube Data API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        """
        self.api_key = api_key
        self.youtube = build('youtube', 'v3', developerKey=api_key)
    
    def get_channel_id_from_username(self, username):
        """
        ì±„ë„ ì‚¬ìš©ìëª…(@username)ìœ¼ë¡œ ì±„ë„ ID ì°¾ê¸°
        """
        try:
            # @ ì œê±°
            username_cleaned = username.replace('@', '')
            
            # 1. handles().list API ì‚¬ìš© (ìµœì‹  ë°©ì‹)
            # ì´ APIëŠ” í•¸ë“¤ë¡œ ì§ì ‘ IDë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            request = self.youtube.channels().list(
                part='id',
                forHandle=username_cleaned
            )
            response = request.execute()
            if 'items' in response and response['items']:
                return response['items'][0]['id']

            # 2. forUsername ì‚¬ìš© (êµ¬í˜• ë°©ì‹)
            request = self.youtube.channels().list(
                part='id',
                forUsername=username_cleaned
            )
            response = request.execute()
            
            if 'items' in response and response['items']:
                return response['items'][0]['id']
            
            # 3. search API ì‚¬ìš© (ìµœí›„ì˜ ìˆ˜ë‹¨)
            request = self.youtube.search().list(
                part='id',
                q=username, # @ê°€ í¬í•¨ëœ ì›ë˜ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰
                type='channel',
                maxResults=1
            )
            response = request.execute()
            
            if 'items' in response and response['items']:
                return response['items'][0]['id']['channelId']
            
            return None
            
        except HttpError as e:
            print(f"  [DataCollector] âŒ ì±„ë„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
    
    def get_channel_info(self, channel_id):
        """
        ì±„ë„ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        """
        try:
            request = self.youtube.channels().list(
                part='snippet,statistics',
                id=channel_id
            )
            response = request.execute()
            
            if 'items' not in response or not response['items']:
                print(f"  [DataCollector] âŒ ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {channel_id}")
                return None
            
            channel = response['items'][0]
            
            return {
                'channel_id': channel_id,
                'channel_name': channel['snippet']['title'],
                'description': channel['snippet']['description'],
                'subscriber_count': int(channel['statistics'].get('subscriberCount', 0)),
                'total_views': int(channel['statistics']['viewCount']),
                'video_count': int(channel['statistics']['videoCount']),
                'published_at': channel['snippet']['publishedAt']
            }
            
        except HttpError as e:
            print(f"  [DataCollector] âŒ HTTP ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            print(f"  [DataCollector] âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return None
    
    def get_channel_videos(self, channel_id, max_results=50, months_back=6):
        """
        ì±„ë„ì˜ ìµœê·¼ ì˜ìƒ ëª©ë¡ ìˆ˜ì§‘
        """
        try:
            # ë¶„ì„ ì‹œì‘ ë‚ ì§œ ê³„ì‚°
            published_after = (datetime.now() - timedelta(days=months_back*30)).isoformat() + 'Z'
            
            video_ids = []
            next_page_token = None
            
            while len(video_ids) < max_results:
                request = self.youtube.search().list(
                    part='id',
                    channelId=channel_id,
                    type='video',
                    order='date', # ìµœì‹ ìˆœ
                    maxResults=min(50, max_results - len(video_ids)), # API ìµœëŒ€ 50ê°œ
                    publishedAfter=published_after,
                    pageToken=next_page_token
                )
                response = request.execute()
                
                video_ids.extend([item['id']['videoId'] for item in response['items']])
                
                next_page_token = response.get('nextPageToken')
                if not next_page_token:
                    break # ë‹¤ìŒ í˜ì´ì§€ ì—†ìœ¼ë©´ ì¢…ë£Œ
            
            return video_ids
            
        except HttpError as e:
            print(f"  [DataCollector] âŒ API ì˜¤ë¥˜: {e}")
            return []
            
    def _get_comment_threads(self, video_id: str, max_comments: int = 100) -> list:
        """[NEW] ì˜ìƒì˜ ìµœìƒìœ„ ëŒ“ê¸€ í…ìŠ¤íŠ¸ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        comments = []
        try:
            request = self.youtube.commentThreads().list(
                part="snippet",
                videoId=video_id,
                maxResults=min(max_comments, 100), # API ìµœëŒ€ 100
                order="relevance", # ê´€ë ¨ì„± ë†’ì€ ëŒ“ê¸€ (ë˜ëŠ” 'time' for ìµœì‹ )
                textFormat="plainText"
            )
            response = request.execute()
            
            for item in response.get('items', []):
                comment_text = item['snippet']['topLevelComment']['snippet']['textDisplay']
                comments.append(comment_text)
                
            return comments
        except HttpError as e:
            # 403: ëŒ“ê¸€ ë¹„í™œì„±í™” ë˜ëŠ” ì ‘ê·¼ ê±°ë¶€
            if e.resp.status == 403:
                print(f"    [DataCollector] âš ï¸ {video_id} ì˜ìƒ ëŒ“ê¸€ ë¹„í™œì„±í™”ë¨.")
            else:
                print(f"    [DataCollector] âŒ {video_id} ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return [] # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        except Exception as e:
            print(f"    [DataCollector] âŒ {video_id} ëŒ“ê¸€ íŒŒì‹± ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e}")
            return []

    def get_video_details(self, video_ids, include_comments=True, max_comments=100):
        """
        [ìˆ˜ì •ë¨] ì˜ìƒ ìƒì„¸ ì •ë³´ + ëŒ“ê¸€ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ë°°ì¹˜ ì²˜ë¦¬)
        """
        videos_data = []
        
        # APIëŠ” í•œ ë²ˆì— ìµœëŒ€ 50ê°œê¹Œì§€ ID ì²˜ë¦¬ ê°€ëŠ¥
        for i in range(0, len(video_ids), 50):
            batch_ids = video_ids[i:i+50]
            
            try:
                # 1. ë¹„ë””ì˜¤ ê¸°ë³¸ ì •ë³´/í†µê³„ ì¼ê´„ ì¡°íšŒ
                request = self.youtube.videos().list(
                    part='snippet,contentDetails,statistics',
                    id=','.join(batch_ids)
                )
                response = request.execute()
                
                for video in response['items']:
                    # ì˜ìƒ ê¸¸ì´(ISO 8601)ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
                    duration_iso = video['contentDetails']['duration']
                    duration_seconds = int(isodate.parse_duration(duration_iso).total_seconds())
                    
                    # ì—…ë¡œë“œ ë‚ ì§œë¡œë¶€í„° ê²½ê³¼ ì¼ìˆ˜ ê³„ì‚°
                    published_at_str = video['snippet']['publishedAt']
                    published_at = datetime.fromisoformat(published_at_str.replace('Z', '+00:00'))
                    days_since_upload = (datetime.now(published_at.tzinfo) - published_at).days
                    if days_since_upload == 0:
                        days_since_upload = 1  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                    
                    video_data = {
                        'video_id': video['id'],
                        'title': video['snippet']['title'],
                        'published_at': published_at_str,
                        'days_since_upload': days_since_upload,
                        'duration_seconds': duration_seconds,
                        'duration_formatted': self._format_duration(duration_seconds),
                        'view_count': int(video['statistics'].get('viewCount', 0)),
                        'like_count': int(video['statistics'].get('likeCount', 0)),
                        'comment_count': int(video['statistics'].get('commentCount', 0)),
                        'tags': video['snippet'].get('tags', []),
                        'thumbnail_high': video['snippet']['thumbnails'].get('high', {}).get('url', ''),
                        'comments': [] # [NEW] ëŒ“ê¸€ í•„ë“œ ì´ˆê¸°í™”
                    }
                    
                    # 2. [NEW] ê°œë³„ ì˜ìƒì˜ ëŒ“ê¸€ ìˆ˜ì§‘ ë¡œì§ ì¶”ê°€
                    if include_comments:
                        # ì´ ë¶€ë¶„ì€ ì˜ìƒ Nê°œë§Œí¼ APIë¥¼ ì¶”ê°€ í˜¸ì¶œí•©ë‹ˆë‹¤.
                        comments_collected = self._get_comment_threads(
                            video['id'], 
                            max_comments=max_comments
                        )
                        video_data['comments'] = comments_collected
                        if len(comments_collected) > 0:
                            print(f"    [DataCollector] ğŸ’¬ {video['id']} ëŒ“ê¸€ {len(comments_collected)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                        else:
                            print(f"    [DataCollector] ğŸ’¬ {video['id']} ëŒ“ê¸€ ìˆ˜ì§‘ ì‹¤íŒ¨ ë˜ëŠ” ëŒ“ê¸€ ì—†ìŒ")
                    
                    videos_data.append(video_data)
                    
            except HttpError as e:
                print(f"  [DataCollector] âŒ API ì˜¤ë¥˜ (Video Batch {i}): {e}")
                continue
        
        return videos_data
    
    def _format_duration(self, seconds):
        """ì˜ìƒ ê¸¸ì´ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        hours = seconds // 3600
        minutes = (seconds % 3600) // 60
        secs = seconds % 60
        
        if hours > 0:
            return f"{hours}:{minutes:02d}:{secs:02d}"
        else:
            return f"{minutes}:{secs:02d}"
    
    def collect_full_data(self, channel_id, max_videos=50, months_back=6):
        """
        [ìˆ˜ì •ë¨] ì±„ë„ì˜ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ (ì›ìŠ¤í†±, ëŒ“ê¸€ í¬í•¨)
        """
        print(f"  [DataCollector] ğŸ“Š ì±„ë„ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        channel_info = self.get_channel_info(channel_id)
        
        if not channel_info:
            print("  [DataCollector] âŒ ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        print(f"  [DataCollector] âœ… ì±„ë„: {channel_info['channel_name']}")
        print(f"     êµ¬ë…ì: {channel_info['subscriber_count']:,}ëª…")
        print(f"     ì´ ì¡°íšŒìˆ˜: {channel_info['total_views']:,}íšŒ")
        
        print(f"\n  [DataCollector] ğŸ¬ ìµœê·¼ {months_back}ê°œì›” ì˜ìƒ ID ìˆ˜ì§‘ ì¤‘...")
        video_ids = self.get_channel_videos(channel_id, max_videos, months_back)
        print(f"  [DataCollector] âœ… ì˜ìƒ {len(video_ids)}ê°œ ë°œê²¬ (ìµœëŒ€ {max_videos}ê°œ)")
        
        if not video_ids:
            print("  [DataCollector] âš ï¸ ë¶„ì„í•  ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return {
                'channel': channel_info,
                'videos': [],
                'collection_date': datetime.now().isoformat(),
                'analysis_period_months': months_back
            }

        print(f"\n  [DataCollector] ğŸ“ ì˜ìƒ ìƒì„¸ ì •ë³´ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘... (ì‹œê°„ ì†Œìš”)")
        videos_data = self.get_video_details(video_ids, include_comments=True, max_comments=100)
        
        # ëŒ“ê¸€ ìˆ˜ì§‘ í†µê³„
        total_comments = sum(len(video.get('comments', [])) for video in videos_data)
        videos_with_comments = sum(1 for video in videos_data if len(video.get('comments', [])) > 0)
        avg_comments = total_comments / len(videos_data) if len(videos_data) > 0 else 0
        
        print(f"  [DataCollector] âœ… {len(videos_data)}ê°œ ì˜ìƒ ì •ë³´ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"  [DataCollector] ğŸ“Š ëŒ“ê¸€ ìˆ˜ì§‘ í†µê³„:")
        print(f"     - ì „ì²´ ìˆ˜ì§‘ ëŒ“ê¸€: {total_comments:,}ê°œ")
        print(f"     - ëŒ“ê¸€ ìˆ˜ì§‘ëœ ì˜ìƒ: {videos_with_comments}/{len(videos_data)}ê°œ ({videos_with_comments/len(videos_data)*100:.1f}%)")
        print(f"     - ì˜ìƒë‹¹ í‰ê·  ëŒ“ê¸€: {avg_comments:.1f}ê°œ")
        
        return {
            'channel': channel_info,
            'videos': videos_data,
            'collection_date': datetime.now().isoformat(),
            'analysis_period_months': months_back
        }
    
    def save_to_json(self, data, filename='channel_data.json'):
        """ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\n  [DataCollector] ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")


# ========================================
# ì‚¬ìš© ì˜ˆì‹œ (ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ê²½ìš°)
# ========================================
if __name__ == "__main__":
    # API í‚¤ ì„¤ì • (ì‹¤ì œ í‚¤ë¡œ ë³€ê²½ í•„ìš”)
    API_KEY = "YOUR_YOUTUBE_API_KEY" # .env ë˜ëŠ” ì§ì ‘ ì…ë ¥
    
    if API_KEY == "YOUR_YOUTUBE_API_KEY":
        print("âŒ 'youtube_data_collector.py' í•˜ë‹¨ì˜ API_KEYë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
    else:
        collector = YouTubeDataCollector(API_KEY)
        
        # @username ë˜ëŠ” ì±„ë„ ID
        channel_query = "@bbomni" 
        
        print(f"ğŸ” ì±„ë„ ID ê²€ìƒ‰ ì¤‘: {channel_query}")
        CHANNEL_ID = channel_query
        
        # UCë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ID ê²€ìƒ‰ ì‹œë„
        if not channel_query.startswith("UC"):
            CHANNEL_ID = collector.get_channel_id_from_username(channel_query)
        
        if not CHANNEL_ID:
            print("âŒ ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"âœ… ì±„ë„ ID ë°œê²¬: {CHANNEL_ID}\n")
            
            data = collector.collect_full_data(
                channel_id=CHANNEL_ID,
                max_videos=50,  # í…ŒìŠ¤íŠ¸ìš© (ìµœëŒ€ 50ê°œ)
                months_back=6   # ìµœê·¼ 6ê°œì›”
            )
            
            if data:
                filename = f"data_{CHANNEL_ID}_{datetime.now().strftime('%Y%m%d')}.json"
                collector.save_to_json(data, filename)
                print(f"\nâœ… ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ!")
                print(f"   ìˆ˜ì§‘ëœ ì˜ìƒ: {len(data['videos'])}ê°œ")
                print(f"   ë‹¤ìŒ ë‹¨ê³„: ì§€í‘œ ê³„ì‚° ëª¨ë“ˆ ì‹¤í–‰ (`{filename}` íŒŒì¼ ì‚¬ìš©)")